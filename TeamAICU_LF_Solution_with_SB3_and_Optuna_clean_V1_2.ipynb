{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "TeamAICU_LF_Solution_with_SB3_and_Optuna_clean_V1_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kC0O7eEtHSo"
      },
      "source": [
        "AICU Lane following with stable baselines3\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDaC4T4WyCMH"
      },
      "source": [
        "Check the recieved GPU (necessary minimum of 16gb GPU RAM)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coMOkQ7IyI1u"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21Xbd1MwtJ9B"
      },
      "source": [
        "Installation section:\n",
        " - Stable baselines 3\n",
        " - Optuna\n",
        " - swig cmake\n",
        " - GYM (OpenAI)\n",
        " - pip (latest version)\n",
        " - GYM Duckietown\n",
        " - Virtual display librarys:\n",
        "  - pyvirtualdisplay\n",
        "  - piglet\n",
        "  - xvfb\n",
        "  - python-opengl\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3CUIW01t_0H"
      },
      "source": [
        "#Install stable baselines 3\n",
        "!pip install stable-baselines3[extra] box2d box2d-kengz\n",
        "\n",
        "#Install Optuna (hyperparam. optimizer tool)\n",
        "!pip install optuna\n",
        "\n",
        "#Install swig, cmake\n",
        "!apt install swig cmake\n",
        "\n",
        "#Clone and install OPENAI - GYM\n",
        "!git clone https://github.com/openai/gym.git\n",
        "%cd gym/\n",
        "!pip3 install -e .\n",
        "\n",
        "#Install and upgrade pip\n",
        "!python3 -m pip install --upgrade pip\n",
        "%cd ..\n",
        "\n",
        "#Clone and install Duckietown-GYM\n",
        "!git clone https://github.com/duckietown/gym-duckietown.git\n",
        "%cd gym-duckietown/\n",
        "!git checkout dcf8dd3\n",
        "!pip install -e .\n",
        "\n",
        "#Create virtual monitor\n",
        "%cd /\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install piglet\n",
        "!apt install xvfb -y\n",
        "!apt-get install python-opengl -y\n",
        "!apt-get install ffmpeg freeglut3-dev xvfb \n",
        "\n",
        "#Start display\n",
        "print(\"\\n Starting virtual display... \\n\")\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(640, 480))\n",
        "display.start()\n",
        "\n",
        "#Install duckietown world for visualization\n",
        "#!git clone https://github.com/duckietown/duckietown-world.git\n",
        "#!git chekcout e069378\n",
        "#%cd duckietown-world/\n",
        "#!pip install -r requirements.txt\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2JAW2_jcBcp"
      },
      "source": [
        "#Chosing the right tensorflow version for compatibility reasons\n",
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M-V3NT_vaV_"
      },
      "source": [
        "Import the necessary librarys:\n",
        "- gym\n",
        "- gym envs\n",
        "- duckietown envs\n",
        "- display\n",
        "- os\n",
        "- numpy\n",
        "- plt\n",
        "- PPO\n",
        "- CnnPolicy\n",
        "- BaseCallback\n",
        "- evaluate policy\n",
        "- OPTUNA\n",
        "- joblib\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZMYRuVuy8hk"
      },
      "source": [
        "%cd .. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TQ1VPgNwGwt"
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%cd content\n",
        "%cd gym\n",
        "from gym import envs\n",
        "import gym\n",
        "\n",
        "%cd ..\n",
        "%cd gym-duckietown/\n",
        "import gym_duckietown.envs.duckietown_env\n",
        "env1 = 'Duckietown-small_loop-v0'   #Small loop map\n",
        "env2 = 'Duckietown-udem1-v0'        #More complex urbun environment\n",
        "env3 = 'Duckietown-straight_road-v0'#Straight road map\n",
        "\n",
        "from stable_baselines3 import PPO, A2C #Algorithm and policy import \n",
        "from stable_baselines3.ppo.policies import CnnPolicy\n",
        "from stable_baselines3.a2c.policies import CnnPolicy\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(640, 480))\n",
        "display.start()\n",
        "\n",
        "import optuna\n",
        "import joblib as joblib\n",
        "from optuna.samplers import TPESampler\n",
        "\n",
        "#For data visualization\n",
        "from google.colab import files\n",
        "\n",
        "#In case you want to save best models automaticly\n",
        "#!pip install pickle-mixin\n",
        "#import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMJnvSTgwrmc"
      },
      "source": [
        "Start the basic training with Stable baselines 3 algorithm\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6OwxcvMwq5r"
      },
      "source": [
        "#Create model\n",
        "model = PPO(CnnPolicy, env1, verbose=0, n_steps=512, batch_size=128, gamma=0.99, \n",
        "            gae_lambda=0.9, n_epochs=20, ent_coef=0.0, sde_sample_freq=4, \n",
        "           max_grad_norm=0.5, vf_coef=0.5, learning_rate=0.00003, use_sde=True, clip_range= 0.4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qwr4NJt0w491"
      },
      "source": [
        "#OPTIONAL\n",
        "#Callback function for debugging, and superviseing\n",
        "\n",
        "\n",
        "#class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
        "    \n",
        "    \n",
        "# #   Callback for saving a model (the check is done every ``check_freq`` steps)\n",
        "# #   based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
        "\n",
        "# #   :param check_freq: (int)\n",
        "# #   :param log_dir: (str) Path to the folder where the model will be saved.\n",
        "# #     It must contains the file created by the ``Monitor`` wrapper.\n",
        "# #   :param verbose: (int)\n",
        "    \n",
        "#    def __init__(self, check_freq: int, log_dir: str, verbose=1):\n",
        "#        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
        "#        self.check_freq = check_freq\n",
        "#        self.log_dir = log_dir\n",
        "#        self.save_path = os.path.join(log_dir, 'best_model')\n",
        "#        self.best_mean_reward = -np.inf\n",
        "\n",
        "#    def _init_callback(self) -> None:\n",
        "#        # Create folder if needed\n",
        "#        if self.save_path is not None:\n",
        "#            os.makedirs(self.save_path, exist_ok=True)\n",
        "\n",
        "#    def _on_step(self) -> bool:\n",
        "#        if self.n_calls % self.check_freq == 0:\n",
        "\n",
        "#          # Retrieve training reward\n",
        "#          x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
        "#          if len(x) > 0:\n",
        "#              # Mean training reward over the last 100 episodes\n",
        "#              mean_reward = np.mean(y[-100:])\n",
        "#              if self.verbose > 0:\n",
        "#                print(f\"Num timesteps: {self.num_timesteps}\")\n",
        "#                print(f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\")\n",
        "\n",
        "#              # New best model, you could save the agent here\n",
        "#              if mean_reward > self.best_mean_reward:\n",
        "#                  self.best_mean_reward = mean_reward\n",
        "#                  # Example for saving best model\n",
        "#                  if self.verbose > 0:\n",
        "#                    print(f\"Saving new best model to {self.save_path}.zip\")\n",
        "#                  self.model.save(self.save_path)\n",
        "\n",
        "#        return True\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIEeFUPHxOKw"
      },
      "source": [
        "#OPTIONAL\n",
        "##Create callback and check every 10 steps\n",
        "#callback = SaveOnBestTrainingRewardCallback(check_freq=10, log_dir='/log_dir')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzMC8XJwxW3b"
      },
      "source": [
        "#Evaluate pretrained model\n",
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10) \n",
        "\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9N0JkxDxpwW"
      },
      "source": [
        "#Train agent\n",
        "#model.learn(total_timesteps=int(100), callback=callback) #With optional Callback function\n",
        "model.learn(total_timesteps=int(100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ini28B48xyaR"
      },
      "source": [
        "#Evaluate trained model\n",
        "environment = model.get_env()\n",
        "mean_reward, std_reward = evaluate_policy(model, environment, n_eval_episodes=10)\n",
        "\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0KJjU_ZCoo0"
      },
      "source": [
        "%cd ..\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAcQX2Qyx15x"
      },
      "source": [
        "Hyperparam tuning with optuna"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3axbygJMx5uN"
      },
      "source": [
        "#PPO optimizer function\n",
        "\n",
        "def optimize_ppo(trial):\n",
        "  #Adjust hyperparams \n",
        "    \"\"\" Learning hyperparamters we want to optimise\"\"\"\n",
        "    return {\n",
        "        'n_steps': int(trial.suggest_int('n_steps', 32, 512)),\n",
        "        'gamma': trial.suggest_loguniform('gamma', 0.9, 0.9999),\n",
        "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-3),\n",
        "        'ent_coef': trial.suggest_loguniform('ent_coef', 1e-8, 1e-1),\n",
        "        'n_epochs': trial.suggest_int('n_epochs',3,5)\n",
        "    }\n",
        "\n",
        "#A2C optimizer fucntion\n",
        "\n",
        "def optimize_a2c(trial):\n",
        "  #Adjust hyperparams \n",
        "    \"\"\" Learning hyperparamters we want to optimise\"\"\"\n",
        "    return {\n",
        "        'n_steps': int(trial.suggest_int('n_steps', 32, 512)),\n",
        "        'gamma': trial.suggest_loguniform('gamma', 0.9, 0.9999),\n",
        "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-3),\n",
        "        'ent_coef': trial.suggest_loguniform('ent_coef', 1e-8, 1e-1),\n",
        "    }\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ox7Q9o1Hx9iN"
      },
      "source": [
        "#Agent optimizer function for PPO\n",
        "\n",
        "def optimize_agent(trial):\n",
        "    \"\"\" Train the model and optimise\n",
        "        Optuna maximises the negative log likelihood, so we\n",
        "        need to negate the reward here\n",
        "    \"\"\"\n",
        "    model_params = optimize_ppo(trial)\n",
        "    env = gym.make('Duckietown-straight_road-v0')\n",
        "    #env = gym.make('Duckietown-small_loop-v0')\n",
        "    #For multiprocessing\n",
        "    #env = SubprocVecEnv([lambda: gym.make('Duckietown-small_loop-v0') for i in range(n_cpu)])\n",
        "    model = PPO(CnnPolicy, env1, verbose=0, **model_params)\n",
        "    model.learn(100)\n",
        "\n",
        "    rewards = []\n",
        "    n_episodes, reward_sum = 0, 0.0\n",
        "\n",
        "    obs = env.reset()\n",
        "    while n_episodes < 100000:\n",
        "        action, _ = model.predict(obs)\n",
        "        obs, reward, done, _ = env.step(action)\n",
        "        reward_sum += reward\n",
        "\n",
        "        if done:\n",
        "            rewards.append(reward_sum)\n",
        "            reward_sum = 0.0\n",
        "            n_episodes += 1\n",
        "            obs = env.reset()\n",
        "\n",
        "    last_reward = np.mean(rewards)\n",
        "    trial.report(-1 * last_reward, n_episodes)\n",
        "\n",
        "    #Handle pruning based on the intermediate value.\n",
        "    if trial.should_prune():\n",
        "      raise optuna.TrialPruned()\n",
        "\n",
        "    return -1 * last_reward\n",
        "\n",
        "#Agent optimizer function for A2C\n",
        "\n",
        "def optimize_agent(trial):\n",
        "    \"\"\" Train the model and optimise\n",
        "        Optuna maximises the negative log likelihood, so we\n",
        "        need to negate the reward here\n",
        "    \"\"\"\n",
        "    model_params = optimize_a2c(trial)\n",
        "    env = gym.make('Duckietown-straight_road-v0')\n",
        "    #env = gym.make('Duckietown-small_loop-v0')\n",
        "    #For multiprocessing\n",
        "    #env = SubprocVecEnv([lambda: gym.make('Duckietown-small_loop-v0') for i in range(n_cpu)])\n",
        "    model = A2C(CnnPolicy, env1, verbose=0, **model_params)\n",
        "    model.learn(100)\n",
        "\n",
        "    rewards = []\n",
        "    n_episodes, reward_sum = 0, 0.0\n",
        "\n",
        "    obs = env.reset()\n",
        "    while n_episodes < 100000:\n",
        "        action, _ = model.predict(obs)\n",
        "        obs, reward, done, _ = env.step(action)\n",
        "        reward_sum += reward\n",
        "\n",
        "        if done:\n",
        "            rewards.append(reward_sum)\n",
        "            reward_sum = 0.0\n",
        "            n_episodes += 1\n",
        "            obs = env.reset()\n",
        "\n",
        "    last_reward = np.mean(rewards)\n",
        "    trial.report(-1 * last_reward, n_episodes)\n",
        "\n",
        "    #Handle pruning based on the intermediate value.\n",
        "    if trial.should_prune():\n",
        "      raise optuna.TrialPruned()\n",
        "\n",
        "    return -1 * last_reward\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LE_3_sJyAQ7"
      },
      "source": [
        "#Setting upt the hyperparameter tuner study with pruner \n",
        "sampler = TPESampler(seed=3) \n",
        "study = optuna.create_study(pruner=optuna.pruners.PercentilePruner(percentile=50,n_startup_trials=20,n_warmup_steps=10000,interval_steps=1),sampler=sampler,)\n",
        "\n",
        "#Start optimization (Also handling NaN cases)\n",
        "study.optimize(optimize_agent, n_trials=50, gc_after_trial=True,catch=(float('nan'),))\n",
        "#study.optimize(optimize_agent, n_trials=1000, n_jobs=4) #\n",
        "study.best_params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFMk5jcf3-4a"
      },
      "source": [
        "#Extracting the params\n",
        "BestParams = study.best_params\n",
        "print(BestParams)\n",
        "b_n_steps = BestParams[\"n_steps\"]\n",
        "b_gamma = BestParams[\"gamma\"]\n",
        "b_learning_rate = BestParams[\"learning_rate\"]\n",
        "b_ent_coef = BestParams[\"ent_coef\"]\n",
        "b_n_epochs = BestParams[\"n_epochs\"]\n",
        "\n",
        "#Creating a model with the calculated hyperparams\n",
        "model = PPO(CnnPolicy, env1, verbose=0, n_epochs=b_n_epochs , ent_coef=b_ent_coef, gamma=b_gamma, learning_rate=b_learning_rate, n_steps=b_n_steps)\n",
        "#modela2c = A2C(CnnPolicy, env1, verbose=0, ent_coef=b_ent_coef, gamma=b_gamma, learning_rate=b_learning_rate, n_steps=b_n_steps)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gVKvOxk0iFc"
      },
      "source": [
        "#Train agent\n",
        "#model.learn(total_timesteps=int(100), callback=callback) #With optional Callback function\n",
        "model.learn(total_timesteps=int(100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9Ut0Czl0Hk4"
      },
      "source": [
        "#Evaluate trained model with hyperparameters\n",
        "mean_reward, std_reward = evaluate_policy(model, env1, n_eval_episodes=100)\n",
        "\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7U-_TzKU0o5Y"
      },
      "source": [
        "Resume Hyperparameter tuning. To be able to optimize better we need more computing capacity so we save and load the optimized models timely (in case of a crash the models were saved this way)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-J9drvi02-UM"
      },
      "source": [
        "for i in range(10):\n",
        "  #study.optimize(optimize_agent, n_trials=10, gc_after_trial=True, catch=(float('nan'),))\n",
        "  study.optimize(optimize_agent, n_trials=20, gc_after_trial=True,catch=(float('NaN'),))\n",
        "  study.best_params\n",
        "  joblib.dump(study, 'PPOstraightV1.pkl')\n",
        "  study = joblib.load('PPOstraightV1.pkl')\n",
        "  print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1n8JV8M3O_K"
      },
      "source": [
        "#Printing the best trial and its values\n",
        "study = joblib.load('/content/PPOstraightV1.pkl')\n",
        "print('Best trial until now:')\n",
        "print(' Value: ', study.best_trial.value)\n",
        "print(' Params: ')\n",
        "for key, value in study.best_trial.params.items():\n",
        "    print(f'    {key}: {value}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQJoqXXE3zZP"
      },
      "source": [
        "#Save tunings into variables\n",
        "study1 = joblib.load('/content/PPOstraightV1.pkl')\n",
        "study2 = joblib.load('/content/studyStraightv2seed3a2c.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbC_gJry6hVB"
      },
      "source": [
        "Visualize the tuning results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nc_rPUGl6lBI"
      },
      "source": [
        "#Creates a list of the hyperparameter importances\n",
        "optuna.importance.get_param_importances(study1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDQgYjFT7DIJ"
      },
      "source": [
        "#Empirical distribution function, cumulative probability\n",
        "#Demonstrates that what is the possibility of given Objective values ranges\n",
        "#Study1 - PPO, Study2 - A2C\n",
        "optuna.visualization.plot_edf([study1, study2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMkRhc-c7rYz"
      },
      "source": [
        "#Connects the hyperparam values of each trial and shows their objective values\n",
        "optuna.visualization.plot_parallel_coordinate(study1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DutnZVgb77iv"
      },
      "source": [
        "#Shows the each hyperparameters cluster\n",
        "optuna.visualization.plot_slice(study1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}